[ Wed Jul  6 10:51:18 2022 ] using warm up, epoch: 5
[ Wed Jul  6 10:53:33 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 10:53:33 2022 ] # Parameters: 2192994
[ Wed Jul  6 10:53:33 2022 ] Training epoch: 1
[ Wed Jul  6 10:56:59 2022 ] using warm up, epoch: 5
[ Wed Jul  6 10:57:16 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 10:57:16 2022 ] # Parameters: 2203106
[ Wed Jul  6 10:57:16 2022 ] Training epoch: 1
[ Wed Jul  6 10:59:47 2022 ] using warm up, epoch: 5
[ Wed Jul  6 11:00:04 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 11:00:04 2022 ] # Parameters: 2203106
[ Wed Jul  6 11:00:04 2022 ] Training epoch: 1
[ Wed Jul  6 11:02:44 2022 ] using warm up, epoch: 5
[ Wed Jul  6 11:03:00 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 11:03:00 2022 ] # Parameters: 2203106
[ Wed Jul  6 11:03:00 2022 ] Training epoch: 1
[ Wed Jul  6 11:03:42 2022 ] using warm up, epoch: 5
[ Wed Jul  6 11:03:58 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 11:03:58 2022 ] # Parameters: 2192882
[ Wed Jul  6 11:03:58 2022 ] Training epoch: 1
[ Wed Jul  6 11:04:16 2022 ] using warm up, epoch: 5
[ Wed Jul  6 11:04:33 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 11:04:33 2022 ] # Parameters: 2192882
[ Wed Jul  6 11:04:33 2022 ] Training epoch: 1
[ Wed Jul  6 11:05:04 2022 ] using warm up, epoch: 5
[ Wed Jul  6 11:05:23 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 11:05:23 2022 ] # Parameters: 2199986
[ Wed Jul  6 11:05:23 2022 ] Training epoch: 1
[ Wed Jul  6 11:06:10 2022 ] using warm up, epoch: 5
[ Wed Jul  6 11:06:26 2022 ] Parameters:
{'work_dir': 'work_dir/ntu120/csub/sym_mod', 'model_saved_name': 'work_dir/ntu120/csub/sym_mod/runs', 'config': 'config/nturgbd120-cross-subject/default.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 32, 'train_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 64, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': False, 'bone': False}, 'test_feeder_args': {'data_path': 'data/ntu120/NTU120_CSub.npz', 'split': 'test', 'window_size': 64, 'p_interval': [0.95], 'vel': False, 'bone': False, 'debug': False}, 'model': 'model.sym_module.Model', 'model_args': {'num_class': 120, 'num_point': 25, 'num_person': 2, 'graph': 'graph.ntu_rgb_d.Graph', 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [35, 55], 'device': [1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 65, 'weight_decay': 0.0004, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5}

[ Wed Jul  6 11:06:26 2022 ] # Parameters: 2199986
[ Wed Jul  6 11:06:26 2022 ] Training epoch: 1
[ Wed Jul  6 11:09:33 2022 ] 	Mean training loss: 3.0973.  Mean training acc: 22.99%.
[ Wed Jul  6 11:09:33 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 11:09:33 2022 ] Eval epoch: 1
[ Wed Jul  6 11:10:20 2022 ] 	Mean test loss of 796 batches: 2.6952337278193563.
[ Wed Jul  6 11:10:21 2022 ] 	Top1: 27.92%
[ Wed Jul  6 11:10:21 2022 ] 	Top5: 64.83%
[ Wed Jul  6 11:10:21 2022 ] Training epoch: 2
[ Wed Jul  6 11:13:28 2022 ] 	Mean training loss: 2.1171.  Mean training acc: 41.18%.
[ Wed Jul  6 11:13:28 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 11:13:28 2022 ] Eval epoch: 2
[ Wed Jul  6 11:14:15 2022 ] 	Mean test loss of 796 batches: 1.8745389204079181.
[ Wed Jul  6 11:14:16 2022 ] 	Top1: 46.70%
[ Wed Jul  6 11:14:16 2022 ] 	Top5: 80.90%
[ Wed Jul  6 11:14:16 2022 ] Training epoch: 3
[ Wed Jul  6 11:17:23 2022 ] 	Mean training loss: 1.6876.  Mean training acc: 51.46%.
[ Wed Jul  6 11:17:23 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 11:17:23 2022 ] Eval epoch: 3
[ Wed Jul  6 11:18:09 2022 ] 	Mean test loss of 796 batches: 2.1983656299174132.
[ Wed Jul  6 11:18:09 2022 ] 	Top1: 42.55%
[ Wed Jul  6 11:18:10 2022 ] 	Top5: 77.88%
[ Wed Jul  6 11:18:10 2022 ] Training epoch: 4
[ Wed Jul  6 11:21:17 2022 ] 	Mean training loss: 1.4271.  Mean training acc: 58.30%.
[ Wed Jul  6 11:21:17 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 11:21:17 2022 ] Eval epoch: 4
[ Wed Jul  6 11:22:04 2022 ] 	Mean test loss of 796 batches: 1.5401550276045823.
[ Wed Jul  6 11:22:04 2022 ] 	Top1: 54.65%
[ Wed Jul  6 11:22:05 2022 ] 	Top5: 86.03%
[ Wed Jul  6 11:22:05 2022 ] Training epoch: 5
[ Wed Jul  6 11:25:13 2022 ] 	Mean training loss: 1.2900.  Mean training acc: 61.95%.
[ Wed Jul  6 11:25:13 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 11:25:13 2022 ] Eval epoch: 5
[ Wed Jul  6 11:26:01 2022 ] 	Mean test loss of 796 batches: 1.903831446515256.
[ Wed Jul  6 11:26:01 2022 ] 	Top1: 50.23%
[ Wed Jul  6 11:26:01 2022 ] 	Top5: 82.03%
[ Wed Jul  6 11:26:02 2022 ] Training epoch: 6
[ Wed Jul  6 11:29:09 2022 ] 	Mean training loss: 1.1395.  Mean training acc: 66.02%.
[ Wed Jul  6 11:29:09 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 11:29:09 2022 ] Eval epoch: 6
[ Wed Jul  6 11:29:58 2022 ] 	Mean test loss of 796 batches: 1.3833615598666609.
[ Wed Jul  6 11:29:59 2022 ] 	Top1: 59.54%
[ Wed Jul  6 11:29:59 2022 ] 	Top5: 88.88%
[ Wed Jul  6 11:29:59 2022 ] Training epoch: 7
[ Wed Jul  6 11:33:07 2022 ] 	Mean training loss: 1.0694.  Mean training acc: 68.10%.
[ Wed Jul  6 11:33:07 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 11:33:07 2022 ] Eval epoch: 7
[ Wed Jul  6 11:33:55 2022 ] 	Mean test loss of 796 batches: 1.3672344430008125.
[ Wed Jul  6 11:33:55 2022 ] 	Top1: 59.26%
[ Wed Jul  6 11:33:56 2022 ] 	Top5: 88.16%
[ Wed Jul  6 11:33:56 2022 ] Training epoch: 8
[ Wed Jul  6 11:37:04 2022 ] 	Mean training loss: 1.0233.  Mean training acc: 69.18%.
[ Wed Jul  6 11:37:04 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 11:37:04 2022 ] Eval epoch: 8
[ Wed Jul  6 11:37:51 2022 ] 	Mean test loss of 796 batches: 1.3288950082944266.
[ Wed Jul  6 11:37:52 2022 ] 	Top1: 62.07%
[ Wed Jul  6 11:37:52 2022 ] 	Top5: 88.30%
[ Wed Jul  6 11:37:52 2022 ] Training epoch: 9
[ Wed Jul  6 11:41:00 2022 ] 	Mean training loss: 0.9795.  Mean training acc: 70.63%.
[ Wed Jul  6 11:41:00 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 11:41:00 2022 ] Eval epoch: 9
[ Wed Jul  6 11:41:48 2022 ] 	Mean test loss of 796 batches: 1.2743401798845535.
[ Wed Jul  6 11:41:48 2022 ] 	Top1: 62.96%
[ Wed Jul  6 11:41:49 2022 ] 	Top5: 89.73%
[ Wed Jul  6 11:41:49 2022 ] Training epoch: 10
[ Wed Jul  6 11:44:56 2022 ] 	Mean training loss: 0.9574.  Mean training acc: 71.13%.
[ Wed Jul  6 11:44:56 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 11:44:56 2022 ] Eval epoch: 10
[ Wed Jul  6 11:45:44 2022 ] 	Mean test loss of 796 batches: 1.1756388696294333.
[ Wed Jul  6 11:45:45 2022 ] 	Top1: 64.04%
[ Wed Jul  6 11:45:45 2022 ] 	Top5: 91.04%
[ Wed Jul  6 11:45:45 2022 ] Training epoch: 11
[ Wed Jul  6 11:48:52 2022 ] 	Mean training loss: 0.9319.  Mean training acc: 71.86%.
[ Wed Jul  6 11:48:52 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 11:48:52 2022 ] Eval epoch: 11
[ Wed Jul  6 11:49:39 2022 ] 	Mean test loss of 796 batches: 1.2948111741986108.
[ Wed Jul  6 11:49:39 2022 ] 	Top1: 62.95%
[ Wed Jul  6 11:49:40 2022 ] 	Top5: 90.16%
[ Wed Jul  6 11:49:40 2022 ] Training epoch: 12
[ Wed Jul  6 11:52:46 2022 ] 	Mean training loss: 0.9069.  Mean training acc: 72.60%.
[ Wed Jul  6 11:52:46 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 11:52:46 2022 ] Eval epoch: 12
[ Wed Jul  6 11:53:33 2022 ] 	Mean test loss of 796 batches: 1.2527431856148208.
[ Wed Jul  6 11:53:33 2022 ] 	Top1: 63.73%
[ Wed Jul  6 11:53:34 2022 ] 	Top5: 89.73%
[ Wed Jul  6 11:53:34 2022 ] Training epoch: 13
[ Wed Jul  6 11:56:39 2022 ] 	Mean training loss: 0.8997.  Mean training acc: 72.88%.
[ Wed Jul  6 11:56:39 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 11:56:39 2022 ] Eval epoch: 13
[ Wed Jul  6 11:57:25 2022 ] 	Mean test loss of 796 batches: 1.3761276047882722.
[ Wed Jul  6 11:57:26 2022 ] 	Top1: 61.90%
[ Wed Jul  6 11:57:26 2022 ] 	Top5: 87.76%
[ Wed Jul  6 11:57:26 2022 ] Training epoch: 14
[ Wed Jul  6 12:00:32 2022 ] 	Mean training loss: 0.8835.  Mean training acc: 73.28%.
[ Wed Jul  6 12:00:32 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:00:32 2022 ] Eval epoch: 14
[ Wed Jul  6 12:01:19 2022 ] 	Mean test loss of 796 batches: 1.1055007810194288.
[ Wed Jul  6 12:01:19 2022 ] 	Top1: 67.77%
[ Wed Jul  6 12:01:20 2022 ] 	Top5: 91.64%
[ Wed Jul  6 12:01:20 2022 ] Training epoch: 15
[ Wed Jul  6 12:04:25 2022 ] 	Mean training loss: 0.8710.  Mean training acc: 73.61%.
[ Wed Jul  6 12:04:25 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:04:25 2022 ] Eval epoch: 15
[ Wed Jul  6 12:05:12 2022 ] 	Mean test loss of 796 batches: 1.6812256617312455.
[ Wed Jul  6 12:05:13 2022 ] 	Top1: 57.03%
[ Wed Jul  6 12:05:13 2022 ] 	Top5: 83.70%
[ Wed Jul  6 12:05:13 2022 ] Training epoch: 16
[ Wed Jul  6 12:08:19 2022 ] 	Mean training loss: 0.8669.  Mean training acc: 73.68%.
[ Wed Jul  6 12:08:19 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:08:19 2022 ] Eval epoch: 16
[ Wed Jul  6 12:09:05 2022 ] 	Mean test loss of 796 batches: 1.0827367325224468.
[ Wed Jul  6 12:09:06 2022 ] 	Top1: 67.99%
[ Wed Jul  6 12:09:06 2022 ] 	Top5: 91.65%
[ Wed Jul  6 12:09:06 2022 ] Training epoch: 17
[ Wed Jul  6 12:12:12 2022 ] 	Mean training loss: 0.8608.  Mean training acc: 74.07%.
[ Wed Jul  6 12:12:12 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:12:12 2022 ] Eval epoch: 17
[ Wed Jul  6 12:12:58 2022 ] 	Mean test loss of 796 batches: 1.1365590904600655.
[ Wed Jul  6 12:12:59 2022 ] 	Top1: 66.41%
[ Wed Jul  6 12:12:59 2022 ] 	Top5: 90.71%
[ Wed Jul  6 12:12:59 2022 ] Training epoch: 18
[ Wed Jul  6 12:16:05 2022 ] 	Mean training loss: 0.8471.  Mean training acc: 74.29%.
[ Wed Jul  6 12:16:05 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:16:05 2022 ] Eval epoch: 18
[ Wed Jul  6 12:16:51 2022 ] 	Mean test loss of 796 batches: 1.091112557031102.
[ Wed Jul  6 12:16:51 2022 ] 	Top1: 67.68%
[ Wed Jul  6 12:16:51 2022 ] 	Top5: 91.71%
[ Wed Jul  6 12:16:52 2022 ] Training epoch: 19
[ Wed Jul  6 12:19:57 2022 ] 	Mean training loss: 0.8404.  Mean training acc: 74.39%.
[ Wed Jul  6 12:19:57 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:19:57 2022 ] Eval epoch: 19
[ Wed Jul  6 12:20:43 2022 ] 	Mean test loss of 796 batches: 1.32287817227481.
[ Wed Jul  6 12:20:43 2022 ] 	Top1: 63.05%
[ Wed Jul  6 12:20:44 2022 ] 	Top5: 88.74%
[ Wed Jul  6 12:20:44 2022 ] Training epoch: 20
[ Wed Jul  6 12:23:50 2022 ] 	Mean training loss: 0.8395.  Mean training acc: 74.64%.
[ Wed Jul  6 12:23:50 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:23:50 2022 ] Eval epoch: 20
[ Wed Jul  6 12:24:36 2022 ] 	Mean test loss of 796 batches: 1.1698168443525256.
[ Wed Jul  6 12:24:37 2022 ] 	Top1: 65.85%
[ Wed Jul  6 12:24:37 2022 ] 	Top5: 91.44%
[ Wed Jul  6 12:24:37 2022 ] Training epoch: 21
[ Wed Jul  6 12:27:43 2022 ] 	Mean training loss: 0.8256.  Mean training acc: 75.07%.
[ Wed Jul  6 12:27:43 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:27:43 2022 ] Eval epoch: 21
[ Wed Jul  6 12:28:30 2022 ] 	Mean test loss of 796 batches: 1.0936613429581101.
[ Wed Jul  6 12:28:30 2022 ] 	Top1: 68.03%
[ Wed Jul  6 12:28:31 2022 ] 	Top5: 91.23%
[ Wed Jul  6 12:28:31 2022 ] Training epoch: 22
[ Wed Jul  6 12:31:37 2022 ] 	Mean training loss: 0.8235.  Mean training acc: 74.91%.
[ Wed Jul  6 12:31:37 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:31:37 2022 ] Eval epoch: 22
[ Wed Jul  6 12:32:23 2022 ] 	Mean test loss of 796 batches: 1.1770121286337698.
[ Wed Jul  6 12:32:23 2022 ] 	Top1: 66.04%
[ Wed Jul  6 12:32:24 2022 ] 	Top5: 90.72%
[ Wed Jul  6 12:32:24 2022 ] Training epoch: 23
[ Wed Jul  6 12:35:31 2022 ] 	Mean training loss: 0.8173.  Mean training acc: 75.23%.
[ Wed Jul  6 12:35:31 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 12:35:31 2022 ] Eval epoch: 23
[ Wed Jul  6 12:36:19 2022 ] 	Mean test loss of 796 batches: 1.3249450954210817.
[ Wed Jul  6 12:36:19 2022 ] 	Top1: 61.54%
[ Wed Jul  6 12:36:20 2022 ] 	Top5: 88.64%
[ Wed Jul  6 12:36:20 2022 ] Training epoch: 24
[ Wed Jul  6 12:39:27 2022 ] 	Mean training loss: 0.8153.  Mean training acc: 75.27%.
[ Wed Jul  6 12:39:27 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 12:39:27 2022 ] Eval epoch: 24
[ Wed Jul  6 12:40:16 2022 ] 	Mean test loss of 796 batches: 1.1420541347645634.
[ Wed Jul  6 12:40:16 2022 ] 	Top1: 67.30%
[ Wed Jul  6 12:40:16 2022 ] 	Top5: 90.92%
[ Wed Jul  6 12:40:16 2022 ] Training epoch: 25
[ Wed Jul  6 12:43:24 2022 ] 	Mean training loss: 0.8040.  Mean training acc: 75.34%.
[ Wed Jul  6 12:43:24 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 12:43:24 2022 ] Eval epoch: 25
[ Wed Jul  6 12:44:12 2022 ] 	Mean test loss of 796 batches: 1.1253225441329444.
[ Wed Jul  6 12:44:12 2022 ] 	Top1: 67.98%
[ Wed Jul  6 12:44:13 2022 ] 	Top5: 91.39%
[ Wed Jul  6 12:44:13 2022 ] Training epoch: 26
[ Wed Jul  6 12:47:21 2022 ] 	Mean training loss: 0.8086.  Mean training acc: 75.57%.
[ Wed Jul  6 12:47:21 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 12:47:21 2022 ] Eval epoch: 26
[ Wed Jul  6 12:48:09 2022 ] 	Mean test loss of 796 batches: 1.0396007908948104.
[ Wed Jul  6 12:48:09 2022 ] 	Top1: 70.03%
[ Wed Jul  6 12:48:10 2022 ] 	Top5: 91.60%
[ Wed Jul  6 12:48:10 2022 ] Training epoch: 27
[ Wed Jul  6 12:51:17 2022 ] 	Mean training loss: 0.8039.  Mean training acc: 75.49%.
[ Wed Jul  6 12:51:17 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 12:51:17 2022 ] Eval epoch: 27
[ Wed Jul  6 12:52:04 2022 ] 	Mean test loss of 796 batches: 1.2758322555440753.
[ Wed Jul  6 12:52:05 2022 ] 	Top1: 64.64%
[ Wed Jul  6 12:52:05 2022 ] 	Top5: 89.24%
[ Wed Jul  6 12:52:05 2022 ] Training epoch: 28
[ Wed Jul  6 12:55:12 2022 ] 	Mean training loss: 0.7994.  Mean training acc: 75.59%.
[ Wed Jul  6 12:55:12 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 12:55:12 2022 ] Eval epoch: 28
[ Wed Jul  6 12:55:59 2022 ] 	Mean test loss of 796 batches: 1.0000677690209456.
[ Wed Jul  6 12:55:59 2022 ] 	Top1: 70.85%
[ Wed Jul  6 12:55:59 2022 ] 	Top5: 92.66%
[ Wed Jul  6 12:55:59 2022 ] Training epoch: 29
[ Wed Jul  6 12:59:05 2022 ] 	Mean training loss: 0.8009.  Mean training acc: 75.72%.
[ Wed Jul  6 12:59:05 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 12:59:05 2022 ] Eval epoch: 29
[ Wed Jul  6 12:59:52 2022 ] 	Mean test loss of 796 batches: 1.0369121835159896.
[ Wed Jul  6 12:59:53 2022 ] 	Top1: 69.35%
[ Wed Jul  6 12:59:53 2022 ] 	Top5: 92.62%
[ Wed Jul  6 12:59:53 2022 ] Training epoch: 30
[ Wed Jul  6 13:03:00 2022 ] 	Mean training loss: 0.7905.  Mean training acc: 76.00%.
[ Wed Jul  6 13:03:00 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 13:03:00 2022 ] Eval epoch: 30
[ Wed Jul  6 13:03:46 2022 ] 	Mean test loss of 796 batches: 1.1712959700418477.
[ Wed Jul  6 13:03:47 2022 ] 	Top1: 66.29%
[ Wed Jul  6 13:03:47 2022 ] 	Top5: 90.26%
[ Wed Jul  6 13:03:47 2022 ] Training epoch: 31
[ Wed Jul  6 13:06:53 2022 ] 	Mean training loss: 0.7929.  Mean training acc: 75.90%.
[ Wed Jul  6 13:06:53 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 13:06:53 2022 ] Eval epoch: 31
[ Wed Jul  6 13:07:40 2022 ] 	Mean test loss of 796 batches: 1.2055465530360763.
[ Wed Jul  6 13:07:40 2022 ] 	Top1: 65.86%
[ Wed Jul  6 13:07:40 2022 ] 	Top5: 90.73%
[ Wed Jul  6 13:07:41 2022 ] Training epoch: 32
[ Wed Jul  6 13:10:46 2022 ] 	Mean training loss: 0.7956.  Mean training acc: 75.89%.
[ Wed Jul  6 13:10:46 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:10:46 2022 ] Eval epoch: 32
[ Wed Jul  6 13:11:33 2022 ] 	Mean test loss of 796 batches: 1.051699587635359.
[ Wed Jul  6 13:11:34 2022 ] 	Top1: 69.80%
[ Wed Jul  6 13:11:34 2022 ] 	Top5: 92.01%
[ Wed Jul  6 13:11:34 2022 ] Training epoch: 33
[ Wed Jul  6 13:14:40 2022 ] 	Mean training loss: 0.7880.  Mean training acc: 75.99%.
[ Wed Jul  6 13:14:40 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:14:40 2022 ] Eval epoch: 33
[ Wed Jul  6 13:15:27 2022 ] 	Mean test loss of 796 batches: 1.0534016421092816.
[ Wed Jul  6 13:15:27 2022 ] 	Top1: 68.35%
[ Wed Jul  6 13:15:28 2022 ] 	Top5: 92.32%
[ Wed Jul  6 13:15:28 2022 ] Training epoch: 34
[ Wed Jul  6 13:18:34 2022 ] 	Mean training loss: 0.7853.  Mean training acc: 76.13%.
[ Wed Jul  6 13:18:34 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:18:34 2022 ] Eval epoch: 34
[ Wed Jul  6 13:19:21 2022 ] 	Mean test loss of 796 batches: 1.0361605839962935.
[ Wed Jul  6 13:19:21 2022 ] 	Top1: 69.58%
[ Wed Jul  6 13:19:22 2022 ] 	Top5: 92.01%
[ Wed Jul  6 13:19:22 2022 ] Training epoch: 35
[ Wed Jul  6 13:22:28 2022 ] 	Mean training loss: 0.7809.  Mean training acc: 76.24%.
[ Wed Jul  6 13:22:28 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:22:28 2022 ] Eval epoch: 35
[ Wed Jul  6 13:23:15 2022 ] 	Mean test loss of 796 batches: 1.0237638614510172.
[ Wed Jul  6 13:23:16 2022 ] 	Top1: 70.03%
[ Wed Jul  6 13:23:16 2022 ] 	Top5: 92.31%
[ Wed Jul  6 13:23:16 2022 ] Training epoch: 36
[ Wed Jul  6 13:26:23 2022 ] 	Mean training loss: 0.4557.  Mean training acc: 85.82%.
[ Wed Jul  6 13:26:23 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 13:26:23 2022 ] Eval epoch: 36
[ Wed Jul  6 13:27:09 2022 ] 	Mean test loss of 796 batches: 0.612723350730823.
[ Wed Jul  6 13:27:10 2022 ] 	Top1: 81.15%
[ Wed Jul  6 13:27:10 2022 ] 	Top5: 96.35%
[ Wed Jul  6 13:27:10 2022 ] Training epoch: 37
[ Wed Jul  6 13:30:16 2022 ] 	Mean training loss: 0.3717.  Mean training acc: 88.30%.
[ Wed Jul  6 13:30:16 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:30:16 2022 ] Eval epoch: 37
[ Wed Jul  6 13:31:03 2022 ] 	Mean test loss of 796 batches: 0.6129376033330383.
[ Wed Jul  6 13:31:03 2022 ] 	Top1: 81.40%
[ Wed Jul  6 13:31:04 2022 ] 	Top5: 96.28%
[ Wed Jul  6 13:31:04 2022 ] Training epoch: 38
[ Wed Jul  6 13:34:09 2022 ] 	Mean training loss: 0.3309.  Mean training acc: 89.63%.
[ Wed Jul  6 13:34:09 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:34:09 2022 ] Eval epoch: 38
[ Wed Jul  6 13:34:56 2022 ] 	Mean test loss of 796 batches: 0.5832825034819357.
[ Wed Jul  6 13:34:57 2022 ] 	Top1: 82.24%
[ Wed Jul  6 13:34:57 2022 ] 	Top5: 96.78%
[ Wed Jul  6 13:34:57 2022 ] Training epoch: 39
[ Wed Jul  6 13:38:03 2022 ] 	Mean training loss: 0.3051.  Mean training acc: 90.47%.
[ Wed Jul  6 13:38:03 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:38:03 2022 ] Eval epoch: 39
[ Wed Jul  6 13:38:50 2022 ] 	Mean test loss of 796 batches: 0.5893918728577582.
[ Wed Jul  6 13:38:50 2022 ] 	Top1: 82.13%
[ Wed Jul  6 13:38:51 2022 ] 	Top5: 96.70%
[ Wed Jul  6 13:38:51 2022 ] Training epoch: 40
[ Wed Jul  6 13:41:56 2022 ] 	Mean training loss: 0.2844.  Mean training acc: 91.07%.
[ Wed Jul  6 13:41:56 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:41:56 2022 ] Eval epoch: 40
[ Wed Jul  6 13:42:43 2022 ] 	Mean test loss of 796 batches: 0.6109296379320736.
[ Wed Jul  6 13:42:43 2022 ] 	Top1: 81.74%
[ Wed Jul  6 13:42:44 2022 ] 	Top5: 96.50%
[ Wed Jul  6 13:42:44 2022 ] Training epoch: 41
[ Wed Jul  6 13:45:49 2022 ] 	Mean training loss: 0.2682.  Mean training acc: 91.67%.
[ Wed Jul  6 13:45:49 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:45:49 2022 ] Eval epoch: 41
[ Wed Jul  6 13:46:36 2022 ] 	Mean test loss of 796 batches: 0.6123843848536811.
[ Wed Jul  6 13:46:36 2022 ] 	Top1: 81.83%
[ Wed Jul  6 13:46:36 2022 ] 	Top5: 96.64%
[ Wed Jul  6 13:46:36 2022 ] Training epoch: 42
[ Wed Jul  6 13:49:43 2022 ] 	Mean training loss: 0.2534.  Mean training acc: 92.05%.
[ Wed Jul  6 13:49:43 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 13:49:43 2022 ] Eval epoch: 42
[ Wed Jul  6 13:50:30 2022 ] 	Mean test loss of 796 batches: 0.6357384181707798.
[ Wed Jul  6 13:50:30 2022 ] 	Top1: 81.67%
[ Wed Jul  6 13:50:30 2022 ] 	Top5: 96.45%
[ Wed Jul  6 13:50:30 2022 ] Training epoch: 43
[ Wed Jul  6 13:53:36 2022 ] 	Mean training loss: 0.2394.  Mean training acc: 92.57%.
[ Wed Jul  6 13:53:36 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:53:36 2022 ] Eval epoch: 43
[ Wed Jul  6 13:54:23 2022 ] 	Mean test loss of 796 batches: 0.6412062069597706.
[ Wed Jul  6 13:54:23 2022 ] 	Top1: 81.58%
[ Wed Jul  6 13:54:24 2022 ] 	Top5: 96.32%
[ Wed Jul  6 13:54:24 2022 ] Training epoch: 44
[ Wed Jul  6 13:57:29 2022 ] 	Mean training loss: 0.2263.  Mean training acc: 92.91%.
[ Wed Jul  6 13:57:29 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 13:57:29 2022 ] Eval epoch: 44
[ Wed Jul  6 13:58:15 2022 ] 	Mean test loss of 796 batches: 0.7025577634591703.
[ Wed Jul  6 13:58:16 2022 ] 	Top1: 80.25%
[ Wed Jul  6 13:58:16 2022 ] 	Top5: 95.96%
[ Wed Jul  6 13:58:16 2022 ] Training epoch: 45
[ Wed Jul  6 14:01:22 2022 ] 	Mean training loss: 0.2222.  Mean training acc: 93.04%.
[ Wed Jul  6 14:01:22 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:01:22 2022 ] Eval epoch: 45
[ Wed Jul  6 14:02:08 2022 ] 	Mean test loss of 796 batches: 0.6703116669689291.
[ Wed Jul  6 14:02:08 2022 ] 	Top1: 81.03%
[ Wed Jul  6 14:02:09 2022 ] 	Top5: 96.10%
[ Wed Jul  6 14:02:09 2022 ] Training epoch: 46
[ Wed Jul  6 14:05:14 2022 ] 	Mean training loss: 0.2168.  Mean training acc: 93.29%.
[ Wed Jul  6 14:05:14 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:05:14 2022 ] Eval epoch: 46
[ Wed Jul  6 14:06:01 2022 ] 	Mean test loss of 796 batches: 0.6820786166280958.
[ Wed Jul  6 14:06:01 2022 ] 	Top1: 80.74%
[ Wed Jul  6 14:06:01 2022 ] 	Top5: 96.20%
[ Wed Jul  6 14:06:01 2022 ] Training epoch: 47
[ Wed Jul  6 14:09:08 2022 ] 	Mean training loss: 0.2119.  Mean training acc: 93.40%.
[ Wed Jul  6 14:09:08 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 14:09:08 2022 ] Eval epoch: 47
[ Wed Jul  6 14:09:54 2022 ] 	Mean test loss of 796 batches: 0.7252078460220566.
[ Wed Jul  6 14:09:55 2022 ] 	Top1: 80.01%
[ Wed Jul  6 14:09:55 2022 ] 	Top5: 95.76%
[ Wed Jul  6 14:09:55 2022 ] Training epoch: 48
[ Wed Jul  6 14:13:02 2022 ] 	Mean training loss: 0.2076.  Mean training acc: 93.59%.
[ Wed Jul  6 14:13:02 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 14:13:02 2022 ] Eval epoch: 48
[ Wed Jul  6 14:13:48 2022 ] 	Mean test loss of 796 batches: 0.7031022085616337.
[ Wed Jul  6 14:13:48 2022 ] 	Top1: 80.37%
[ Wed Jul  6 14:13:49 2022 ] 	Top5: 96.05%
[ Wed Jul  6 14:13:49 2022 ] Training epoch: 49
[ Wed Jul  6 14:16:55 2022 ] 	Mean training loss: 0.2006.  Mean training acc: 93.71%.
[ Wed Jul  6 14:16:55 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 14:16:55 2022 ] Eval epoch: 49
[ Wed Jul  6 14:17:42 2022 ] 	Mean test loss of 796 batches: 0.7211420870942986.
[ Wed Jul  6 14:17:42 2022 ] 	Top1: 80.08%
[ Wed Jul  6 14:17:43 2022 ] 	Top5: 95.76%
[ Wed Jul  6 14:17:43 2022 ] Training epoch: 50
[ Wed Jul  6 14:20:48 2022 ] 	Mean training loss: 0.1965.  Mean training acc: 93.95%.
[ Wed Jul  6 14:20:48 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:20:48 2022 ] Eval epoch: 50
[ Wed Jul  6 14:21:35 2022 ] 	Mean test loss of 796 batches: 0.7623296159895221.
[ Wed Jul  6 14:21:35 2022 ] 	Top1: 79.34%
[ Wed Jul  6 14:21:35 2022 ] 	Top5: 95.39%
[ Wed Jul  6 14:21:35 2022 ] Training epoch: 51
[ Wed Jul  6 14:24:41 2022 ] 	Mean training loss: 0.2048.  Mean training acc: 93.57%.
[ Wed Jul  6 14:24:41 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:24:41 2022 ] Eval epoch: 51
[ Wed Jul  6 14:25:29 2022 ] 	Mean test loss of 796 batches: 0.7408679471393327.
[ Wed Jul  6 14:25:29 2022 ] 	Top1: 79.66%
[ Wed Jul  6 14:25:30 2022 ] 	Top5: 95.45%
[ Wed Jul  6 14:25:30 2022 ] Training epoch: 52
[ Wed Jul  6 14:28:37 2022 ] 	Mean training loss: 0.2005.  Mean training acc: 93.79%.
[ Wed Jul  6 14:28:37 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 14:28:37 2022 ] Eval epoch: 52
[ Wed Jul  6 14:29:24 2022 ] 	Mean test loss of 796 batches: 0.727445781755088.
[ Wed Jul  6 14:29:24 2022 ] 	Top1: 80.26%
[ Wed Jul  6 14:29:25 2022 ] 	Top5: 95.83%
[ Wed Jul  6 14:29:25 2022 ] Training epoch: 53
[ Wed Jul  6 14:32:32 2022 ] 	Mean training loss: 0.1982.  Mean training acc: 93.85%.
[ Wed Jul  6 14:32:32 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 14:32:32 2022 ] Eval epoch: 53
[ Wed Jul  6 14:33:19 2022 ] 	Mean test loss of 796 batches: 0.7737112734560392.
[ Wed Jul  6 14:33:19 2022 ] 	Top1: 79.51%
[ Wed Jul  6 14:33:19 2022 ] 	Top5: 95.42%
[ Wed Jul  6 14:33:20 2022 ] Training epoch: 54
[ Wed Jul  6 14:36:26 2022 ] 	Mean training loss: 0.2017.  Mean training acc: 93.70%.
[ Wed Jul  6 14:36:26 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 14:36:26 2022 ] Eval epoch: 54
[ Wed Jul  6 14:37:13 2022 ] 	Mean test loss of 796 batches: 0.7313975524718888.
[ Wed Jul  6 14:37:13 2022 ] 	Top1: 80.13%
[ Wed Jul  6 14:37:13 2022 ] 	Top5: 95.88%
[ Wed Jul  6 14:37:13 2022 ] Training epoch: 55
[ Wed Jul  6 14:40:20 2022 ] 	Mean training loss: 0.1901.  Mean training acc: 94.08%.
[ Wed Jul  6 14:40:20 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 14:40:20 2022 ] Eval epoch: 55
[ Wed Jul  6 14:41:08 2022 ] 	Mean test loss of 796 batches: 0.7627925043317241.
[ Wed Jul  6 14:41:08 2022 ] 	Top1: 79.94%
[ Wed Jul  6 14:41:08 2022 ] 	Top5: 95.51%
[ Wed Jul  6 14:41:09 2022 ] Training epoch: 56
[ Wed Jul  6 14:44:14 2022 ] 	Mean training loss: 0.1133.  Mean training acc: 96.93%.
[ Wed Jul  6 14:44:14 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:44:14 2022 ] Eval epoch: 56
[ Wed Jul  6 14:45:01 2022 ] 	Mean test loss of 796 batches: 0.6641041791749809.
[ Wed Jul  6 14:45:01 2022 ] 	Top1: 82.10%
[ Wed Jul  6 14:45:01 2022 ] 	Top5: 96.36%
[ Wed Jul  6 14:45:01 2022 ] Training epoch: 57
[ Wed Jul  6 14:48:07 2022 ] 	Mean training loss: 0.0867.  Mean training acc: 97.82%.
[ Wed Jul  6 14:48:07 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:48:07 2022 ] Eval epoch: 57
[ Wed Jul  6 14:48:53 2022 ] 	Mean test loss of 796 batches: 0.6701211705850671.
[ Wed Jul  6 14:48:54 2022 ] 	Top1: 82.27%
[ Wed Jul  6 14:48:54 2022 ] 	Top5: 96.25%
[ Wed Jul  6 14:48:54 2022 ] Training epoch: 58
[ Wed Jul  6 14:52:00 2022 ] 	Mean training loss: 0.0765.  Mean training acc: 98.14%.
[ Wed Jul  6 14:52:00 2022 ] 	Time consumption: [Data]03%, [Network]97%
[ Wed Jul  6 14:52:00 2022 ] Eval epoch: 58
[ Wed Jul  6 14:52:47 2022 ] 	Mean test loss of 796 batches: 0.6656131482397641.
[ Wed Jul  6 14:52:47 2022 ] 	Top1: 82.40%
[ Wed Jul  6 14:52:47 2022 ] 	Top5: 96.25%
[ Wed Jul  6 14:52:48 2022 ] Training epoch: 59
[ Wed Jul  6 14:55:53 2022 ] 	Mean training loss: 0.0691.  Mean training acc: 98.35%.
[ Wed Jul  6 14:55:53 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:55:53 2022 ] Eval epoch: 59
[ Wed Jul  6 14:56:40 2022 ] 	Mean test loss of 796 batches: 0.6814171872359125.
[ Wed Jul  6 14:56:40 2022 ] 	Top1: 82.12%
[ Wed Jul  6 14:56:41 2022 ] 	Top5: 96.15%
[ Wed Jul  6 14:56:41 2022 ] Training epoch: 60
[ Wed Jul  6 14:59:47 2022 ] 	Mean training loss: 0.0655.  Mean training acc: 98.47%.
[ Wed Jul  6 14:59:47 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 14:59:47 2022 ] Eval epoch: 60
[ Wed Jul  6 15:00:33 2022 ] 	Mean test loss of 796 batches: 0.6867479262920331.
[ Wed Jul  6 15:00:34 2022 ] 	Top1: 82.06%
[ Wed Jul  6 15:00:34 2022 ] 	Top5: 96.19%
[ Wed Jul  6 15:00:34 2022 ] Training epoch: 61
[ Wed Jul  6 15:03:40 2022 ] 	Mean training loss: 0.0611.  Mean training acc: 98.64%.
[ Wed Jul  6 15:03:40 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 15:03:40 2022 ] Eval epoch: 61
[ Wed Jul  6 15:04:27 2022 ] 	Mean test loss of 796 batches: 0.6884561702328261.
[ Wed Jul  6 15:04:27 2022 ] 	Top1: 82.25%
[ Wed Jul  6 15:04:27 2022 ] 	Top5: 96.20%
[ Wed Jul  6 15:04:27 2022 ] Training epoch: 62
[ Wed Jul  6 15:07:33 2022 ] 	Mean training loss: 0.0586.  Mean training acc: 98.59%.
[ Wed Jul  6 15:07:33 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 15:07:33 2022 ] Eval epoch: 62
[ Wed Jul  6 15:08:19 2022 ] 	Mean test loss of 796 batches: 0.6958047770076061.
[ Wed Jul  6 15:08:20 2022 ] 	Top1: 82.05%
[ Wed Jul  6 15:08:20 2022 ] 	Top5: 96.07%
[ Wed Jul  6 15:08:20 2022 ] Training epoch: 63
[ Wed Jul  6 15:11:26 2022 ] 	Mean training loss: 0.0557.  Mean training acc: 98.74%.
[ Wed Jul  6 15:11:26 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 15:11:26 2022 ] Eval epoch: 63
[ Wed Jul  6 15:12:13 2022 ] 	Mean test loss of 796 batches: 0.6964697418781232.
[ Wed Jul  6 15:12:14 2022 ] 	Top1: 82.19%
[ Wed Jul  6 15:12:14 2022 ] 	Top5: 96.10%
[ Wed Jul  6 15:12:14 2022 ] Training epoch: 64
[ Wed Jul  6 15:15:20 2022 ] 	Mean training loss: 0.0549.  Mean training acc: 98.72%.
[ Wed Jul  6 15:15:20 2022 ] 	Time consumption: [Data]02%, [Network]97%
[ Wed Jul  6 15:15:20 2022 ] Eval epoch: 64
[ Wed Jul  6 15:16:07 2022 ] 	Mean test loss of 796 batches: 0.6948900189643828.
[ Wed Jul  6 15:16:07 2022 ] 	Top1: 82.18%
[ Wed Jul  6 15:16:08 2022 ] 	Top5: 96.08%
[ Wed Jul  6 15:16:08 2022 ] Training epoch: 65
[ Wed Jul  6 15:19:16 2022 ] 	Mean training loss: 0.0516.  Mean training acc: 98.86%.
[ Wed Jul  6 15:19:16 2022 ] 	Time consumption: [Data]03%, [Network]96%
[ Wed Jul  6 15:19:16 2022 ] Eval epoch: 65
[ Wed Jul  6 15:20:03 2022 ] 	Mean test loss of 796 batches: 0.6991664381725854.
[ Wed Jul  6 15:20:04 2022 ] 	Top1: 82.18%
[ Wed Jul  6 15:20:04 2022 ] 	Top5: 96.06%
[ Wed Jul  6 15:20:54 2022 ] Best accuracy: 0.8239949724071565
[ Wed Jul  6 15:20:54 2022 ] Epoch number: 58
[ Wed Jul  6 15:20:54 2022 ] Model name: work_dir/ntu120/csub/sym_mod
[ Wed Jul  6 15:20:54 2022 ] Model total number of params: 2199986
[ Wed Jul  6 15:20:54 2022 ] Weight decay: 0.0004
[ Wed Jul  6 15:20:54 2022 ] Base LR: 0.1
[ Wed Jul  6 15:20:54 2022 ] Batch Size: 64
[ Wed Jul  6 15:20:54 2022 ] Test Batch Size: 64
[ Wed Jul  6 15:20:54 2022 ] seed: 1
